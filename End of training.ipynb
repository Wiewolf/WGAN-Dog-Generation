{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.models import Generator, Critic, WeightClipper\n",
    "hparams = {\n",
    "    \"batch_size\": 2048,\n",
    "    \"learning_rate_gen\": 0.00005,\n",
    "    \"learning_rate_crit\": 0.00005,\n",
    "    \"latent_dim\": 100,\n",
    "    \"device\": device,\n",
    "    \"epochs\": 150,\n",
    "    \"critic_iterations\" : 2,\n",
    "    \"betas\": (0.0, 0.9),\n",
    "\n",
    "\n",
    "\n",
    "    #28,28 convolutionwerte passen hier noch nicht\n",
    "    \"dc1_num\": 8,\n",
    "    \"dc1_size\": 6,\n",
    "    \"dc1_stride\": 2,#12\n",
    "    \"dc1p_size\": 4,\n",
    "    \"dc1p_stride\": 1,\n",
    "    \"dc2_num\": 16,\n",
    "    \"dc2_size\": 2,\n",
    "    \"dc2_stride\": 2,#6\n",
    "    \"dc2p_size\": 2,\n",
    "    \"dc2p_stride\": 1,\n",
    "    \"dc3_num\": 32,\n",
    "    \"dc3_size\": 3,\n",
    "    \"dc3_stride\": 1,#4\n",
    "    \"dc3p_size\": 3,\n",
    "    \"dc3p_stride\": 1,\n",
    "    \"dc4_num\": 64,\n",
    "    \"dc4_size\": 2,\n",
    "    \"dc4_stride\": 2,#2\n",
    "    \"dc4p_size\": 3,\n",
    "    \"dc4p_stride\": 1,\n",
    "    \"dc5_num\": 128,\n",
    "    \"dc5_size\": 2,\n",
    "    \"dc5_stride\": 1,#1\n",
    "\n",
    "    \"dl1\": 64,\n",
    "    \"dl2\": 64,\n",
    "    \"dl3\": 64,\n",
    "\n",
    "\n",
    "    \"gl1\": 100,\n",
    "    \"gl2\": 300,\n",
    "    \"gl3\": 900,\n",
    "    \"gl4\": 1600,\n",
    "    \"upscale_to\": 40, #should be the sqrt of gl4\n",
    "    #40x40\n",
    "    \"gc1_num\": 1,\n",
    "    \"gc1_size\": 4,\n",
    "    \"gc1_stride\": 1,\n",
    "    \"gc1_out_channels\": 512,#n x 64 x 4 x 4\n",
    "    \"gc1p_size\": 4,\n",
    "    \"gc1p_stride\": 2, \n",
    "    \"gc2_num\": 1,\n",
    "    \"gc2_size\": 4,\n",
    "    \"gc2_stride\": 2,\n",
    "    \"gc2_out_channels\": 256, #6 + 3 + 1 = 10 -> 32 x 10 x 10\n",
    "    \"gc2p_size\": 3,\n",
    "    \"gc2p_stride\": 1,\n",
    "    \"gc3_num\": 1,\n",
    "    \"gc3_size\": 4,#\n",
    "    \"gc3_stride\": 2,\n",
    "    \"gc3_out_channels\": 128, #18 + 3 + 1 = 22 -> 16 x 22 x 22\n",
    "    \"gc3p_size\": 3,\n",
    "    \"gc3p_stride\": 1,\n",
    "    \"gc4_num\": 1,\n",
    "    \"gc4_size\": 4,\n",
    "    \"gc4_stride\": 1,\n",
    "    \"gc4_out_channels\": 64, #22 + 3  = 25 -> 8 x 25 x 25\n",
    "    \"gc5_size\": 4,\n",
    "    \"gc5_stride\": 1,\n",
    "    \"gc5_out_channels\": 1, #25 + 3  = 28 -> 1 x 28 x 28\n",
    "    \n",
    "\n",
    "}\n",
    "generator = Generator(hparams).to(device)\n",
    "critic = Critic(hparams).to(device)\n",
    "clipper = WeightClipper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "generator = Generator(hparams).to(device)\n",
    "critic = Critic(hparams).to(device)\n",
    "\n",
    "def gradient_penalty(critic, real, fake, device=\"cpu\"):\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "    alpha = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    interpolated_images = real * alpha + fake * (1 - alpha)\n",
    "    interpolated_images.requires_grad_(True)\n",
    "    mixed_scores = critic(interpolated_images)\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty\n",
    "\n",
    "\n",
    "def create_tqdm_bar(iterable, desc):\n",
    "    return tqdm(enumerate(iterable),total=len(iterable), ncols=150, desc=desc)\n",
    "\n",
    "\n",
    "def train_model_gp(generator, critic, clipper, train_loader, val_loader, tb_logger, hparams, name=\"WGAN\", lambda_gp = 10):\n",
    "    epochs = hparams.get(\"epochs\", 30)\n",
    "    batch_size = torch.tensor(hparams[\"batch_size\"]).to(device)\n",
    "    optimizer_generator = torch.optim.Adam(generator.parameters(), hparams[\"learning_rate_gen\"], betas=hparams[\"betas\"])\n",
    "    optimizer_critic = torch.optim.Adam(critic.parameters(), hparams[\"learning_rate_crit\"], betas=hparams[\"betas\"])\n",
    "    #scheduler_generator = torch.optim.lr_scheduler.StepLR(optimizer_generator, step_size=int(epochs * len(train_loader) / 5), gamma=hparams.get('gamma', 0.9))\n",
    "    #scheduler_critic = torch.optim.lr_scheduler.StepLR(optimizer_critic, step_size=int(epochs * len(train_loader) / 5), gamma=hparams.get('gamma', 0.9))\n",
    "    i = 0\n",
    "    z = 0\n",
    "    for epoch in range(epochs):\n",
    "        generator.train()\n",
    "        critic.train()\n",
    "        training_loss_g = []\n",
    "        training_loss_c = []\n",
    "        validation_loss_g = []\n",
    "        validation_loss_c = []\n",
    "        training_loop = create_tqdm_bar(train_loader, desc=f'Training Epoch [{epoch + 1}/{epochs}]')\n",
    "        for train_iteration,batch in training_loop:\n",
    "            optimizer_critic.zero_grad()\n",
    "            optimizer_generator.zero_grad()\n",
    "            batch = batch.to(device)\n",
    "            fake_batch = torch.randn(size= (batch.shape[0], hparams[\"latent_dim\"])).to(device)\n",
    "            fake_images = generator(fake_batch).detach()\n",
    "            pred_real = torch.squeeze(critic(batch)).to(device) #now shape (batch_size)\n",
    "            pred_fake = torch.squeeze(critic(fake_images)).to(device)  #should be same shape\n",
    "            gradient_penalty_ = gradient_penalty(critic, batch, fake_images, device=device)\n",
    "            loss_c = pred_fake.mean() - pred_real.mean() + lambda_gp * gradient_penalty_\n",
    "            #save loss without gradient penalty for logging\n",
    "            loss_c_no_gp = pred_fake.mean() - pred_real.mean()\n",
    "            loss_c.backward()\n",
    "            optimizer_critic.step()\n",
    "            #append loss without gradient penalty to logging\n",
    "            training_loss_c.append(loss_c_no_gp.item())\n",
    "            training_loop.set_postfix(curr_train_loss = \"{:.8f}\".format(np.mean(training_loss_c)),\n",
    "                                      lr = \"{:.8f}\".format(optimizer_critic.param_groups[0]['lr']))\n",
    "            tb_logger.add_scalar(f'Critic_{name}/train_loss', loss_c.item(), epoch * len(train_loader) + train_iteration)\n",
    "            \n",
    "            if train_iteration % 7 == 1:\n",
    "                with torch.no_grad():\n",
    "                        tb_logger.add_figure(\"Generator:\", plot_generator(generator,fake_batch), epoch * len(train_loader) + train_iteration)\n",
    "            i += 1\n",
    "            if i == hparams[\"critic_iterations\"]:\n",
    "                z += 1\n",
    "                optimizer_critic.zero_grad()\n",
    "                optimizer_generator.zero_grad()\n",
    "                fake_batch = torch.randn(size= (hparams[\"batch_size\"], hparams[\"latent_dim\"])).to(device)\n",
    "                pred_fake = torch.squeeze(critic(generator(fake_batch))).to(device)\n",
    "                loss_g = -torch.mean(pred_fake)\n",
    "                loss_g.backward()\n",
    "                optimizer_generator.step()\n",
    "                #scheduler_generator.step()\n",
    "                training_loss_g.append(loss_g.item())\n",
    "                tb_logger.add_scalar(f'Generator_{name}/train_loss', loss_g.item(), z)\n",
    "                i = 0\n",
    "\n",
    "        generator.eval()\n",
    "        critic.eval()\n",
    "        val_loop = create_tqdm_bar(val_loader, desc=f'Validation Epoch [{epoch + 1}/{epochs}]')\n",
    "        with torch.no_grad():\n",
    "            for val_iterartion, batch in val_loop:\n",
    "                batch = batch.to(device)\n",
    "                fake_batch = torch.randn(size= (hparams[\"batch_size\"], hparams[\"latent_dim\"])).to(device)\n",
    "                pred_real = torch.squeeze(critic(batch)).to(device) #now shape (batch_size)\n",
    "                pred_fake = torch.squeeze(critic(generator(fake_batch))).to(device)  #should be same shape\n",
    "                loss_c = -torch.mean(pred_real) + torch.mean(pred_fake)\n",
    "                validation_loss_c.append(loss_c.item())\n",
    "                training_loop.set_postfix(curr_train_loss = \"{:.8f}\".format(np.mean(training_loss_c)),\n",
    "                                          lr = \"{:.8f}\".format(optimizer_critic.param_groups[0]['lr']))\n",
    "                tb_logger.add_scalar(f'Critic_{name}/val_loss', loss_c.item(), epoch * len(train_loader) + train_iteration)\n",
    "                loss_g = -torch.mean(pred_fake)\n",
    "                validation_loss_g.append(loss_g.item())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path = \"logs\"\n",
    "num_of_runs = len(os.listdir(path)) if os.path.exists(path) else 0\n",
    "path = os.path.join(path, f'run_{num_of_runs + 1}')\n",
    "\n",
    "tb_logger = SummaryWriter(path)\n",
    "\n",
    "# Train the classifier.\n",
    "unlabled_train_loader = torch.utils.data.DataLoader(unlabeled_train, batch_size=hparams['batch_size'], shuffle=True)\n",
    "unlabled_val_loader = torch.utils.data.DataLoader(unlabeled_val, batch_size=hparams['batch_size'], shuffle=False)\n",
    "train_model_gp(generator, critic, clipper, unlabled_train_loader, unlabled_val_loader, tb_logger, hparams= hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.eval()\n",
    "x = torch.randn(size= (hparams[\"batch_size\"], hparams[\"latent_dim\"])).to(device)\n",
    "preds = generator(x).cpu().detach().numpy()\n",
    "for i in range(64):\n",
    "    plt.subplot(8,8,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(preds[i][0], cmap='gray', interpolation='none')\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
